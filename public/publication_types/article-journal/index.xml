<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Article-Journal | Vassilis C. Nicodemou</title>
    <link>https://users.ics.forth.gr/~nikodim/publication_types/article-journal/</link>
      <atom:link href="https://users.ics.forth.gr/~nikodim/publication_types/article-journal/index.xml" rel="self" type="application/rss+xml" />
    <description>Article-Journal</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Fri, 15 Nov 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://users.ics.forth.gr/~nikodim/media/icon_hu_5b01a3c72805cea.png</url>
      <title>Article-Journal</title>
      <link>https://users.ics.forth.gr/~nikodim/publication_types/article-journal/</link>
    </image>
    
    <item>
      <title>Refraction-Aware Structure from Motion for Airborne Bathymetry</title>
      <link>https://users.ics.forth.gr/~nikodim/publication/makris-2024-refraction/</link>
      <pubDate>Fri, 15 Nov 2024 00:00:00 +0000</pubDate>
      <guid>https://users.ics.forth.gr/~nikodim/publication/makris-2024-refraction/</guid>
      <description>&lt;p&gt;In this work, we introduce the first pipeline that combines a refraction-aware structure from motion (SfM) method with a deep learning model specifically designed for airborne bathymetry. We accurately estimate the 3D positions of the submerged points by integrating refraction geometry within the SfM optimization problem. This way, no refraction correction as post-processing is required. Experiments with simulated data that approach real-world capturing conditions demonstrate that SfM with refraction correction is extremely accurate, with submillimeter errors. We integrate our refraction-aware SfM within a deep learning framework that also takes into account radiometrical information, developing a combined spectral and geometry-based approach, with further improvements in accuracy and robustness to different seafloor types, both textured and textureless. We conducted experiments with real-world data at two locations in the southern Mediterranean Sea, with varying seafloor types, which demonstrate the benefits of refraction correction for the deep learning framework. We made our refraction-aware SfM open source, providing researchers in airborne bathymetry with a practical tool to apply SfM in shallow water areas.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Integration of Photogrammetric and Spectral Techniques for Advanced Drone-Based Bathymetry Retrieval Using a Deep Learning Approach</title>
      <link>https://users.ics.forth.gr/~nikodim/publication/alevizos-2022-integration/</link>
      <pubDate>Wed, 24 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://users.ics.forth.gr/~nikodim/publication/alevizos-2022-integration/</guid>
      <description>&lt;p&gt;Shallow bathymetry mapping using proximal sensing techniques is an active field of research that offers a new perspective in studying the seafloor. Drone-based imagery with centimeter resolution allows for bathymetry retrieval in unprecedented detail in areas with adequate water transparency. The majority of studies apply either spectral or photogrammetric techniques for deriving bathymetry from remotely sensed imagery. However, spectral methods require a certain amount of ground-truth depth data for model calibration, while photogrammetric methods cannot perform on texture-less seafloor types. The presented approach takes advantage of the interrelation of the two methods, in order to predict bathymetry in a more efficient way. Thus, we combine structure-from-motion (SfM) outputs along with band-ratios of radiometrically corrected drone images within a specially designed deep convolutional neural network (CNN) that outputs a reliable and robust bathymetry estimation. To achieve effective training of our deep learning system, we utilize interpolated uncrewed surface vehicle (USV) sonar measurements. We perform several predictions at three locations in the southern Mediterranean Sea, with varying seafloor types. Our results show low root-mean-square errors over all study areas (average RMSE ≅ 0.3 m), when the method was trained and tested on the same area each time. In addition, we obtain promising cross-validation performance across different study areas (average RMSE ≅ 0.9 m), which demonstrates the potential of our proposed approach in terms of generalization capabilities on unseen data. Furthermore, areas with mixed seafloor types are suitable for building a model that can be applied in similar locations where only drone data is available.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ambient intelligence in the living room</title>
      <link>https://users.ics.forth.gr/~nikodim/publication/leonidis-2019-ambient/</link>
      <pubDate>Sat, 16 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://users.ics.forth.gr/~nikodim/publication/leonidis-2019-ambient/</guid>
      <description>&lt;p&gt;The emergence of the Ambient Intelligence (AmI) paradigm and the proliferation of Internet of Things (IoT) devices and services unveiled new potentials for the domain of domestic living, where the line between “the computer” and the (intelligent) environment becomes altogether invisible. Particularly, the residents of a house can use the living room not only as a traditional social and individual space where many activities take place, but also as a smart ecosystem that (a) enhances leisure activities by providing a rich suite of entertainment applications, (b) implements a home control middleware, (c) acts as an intervention host that is able to display appropriate content when the users need help or support, (d) behaves as an intelligent agent that communicates with the users in a natural manner and assists them throughout their daily activities, (e) presents a notification hub that provides personalized alerts according to contextual information, and (f) becomes an intermediary communication center for the family. This paper (i) describes how the “Intelligent Living Room” realizes these newly emerged roles, (ii) presents the process that was followed in order to design the living room environment, (iii) introduces the hardware and software facilities that were developed in order to improve quality of life, and (iv) reports the findings of various evaluation experiments conducted to assess the overall User Experience (UX).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Single-shot 3D hand pose estimation using radial basis function networks trained on synthetic data</title>
      <link>https://users.ics.forth.gr/~nikodim/publication/nicodemou-2020-single/</link>
      <pubDate>Wed, 27 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://users.ics.forth.gr/~nikodim/publication/nicodemou-2020-single/</guid>
      <description>&lt;p&gt;In this work, we present a novel framework to perform single-shot hand pose estimation using depth data as input. The method follows a coarse to fine strategy and employs several radial basis function networks (RBFNs) that are trained on a dataset containing only synthetically generated depth maps. Thus, compared to most contemporary deep learning approaches, it does not require the laborious annotation of large, real-world datasets. At run time, an initialization RBFN is used to provide a rough estimation of the hand’s 3D pose. Subsequently, several specialized RBFNs are employed to improve that initial estimation in an iterative refinement scheme. To train the RBFNs, we select a set of hand poses from a real-world sequence that are as diverse as possible. We use this representative set, along with a dense sampling of all possible rotations, as a seed to generate a large synthetic training set. The method is parallelizable, taking advantage of the inherent data parallelism of RBFNs. Furthermore, the method requires few real-world data and virtually no manual annotation. We perform a quantitative evaluation of our method on a testing sequence of our own. We also present quantitative and qualitative results on a public dataset that is commonly used to evaluate hand pose estimation and tracking methods. We show that in all cases, our approach achieves promising results. Moreover, it can achieve comparable or even faster computational performance than current deep learning approaches but on a single CPU core, i.e., without requiring GPU processing.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
